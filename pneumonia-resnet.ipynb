{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-03T21:18:26.767920Z","iopub.status.busy":"2023-05-03T21:18:26.767576Z","iopub.status.idle":"2023-05-03T21:18:26.774339Z","shell.execute_reply":"2023-05-03T21:18:26.773453Z","shell.execute_reply.started":"2023-05-03T21:18:26.767891Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torchvision\n","import numpy as np\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.datasets import ImageFolder\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","# Define dataset paths\n","data_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray'\n","train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T21:18:26.776851Z","iopub.status.busy":"2023-05-03T21:18:26.776249Z","iopub.status.idle":"2023-05-03T21:56:44.557641Z","shell.execute_reply":"2023-05-03T21:56:44.556606Z","shell.execute_reply.started":"2023-05-03T21:18:26.776819Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/15: 100%|██████████| 261/261 [01:58<00:00,  2.19it/s, acc=327, loss=0.031]   \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15: Train Loss: 0.1415, Train Acc: 0.9415, Val Loss: 0.0536, Val Acc: 0.9799\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/15: 100%|██████████| 261/261 [01:59<00:00,  2.19it/s, acc=0.984, loss=0.301]   \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/15: Train Loss: 0.0406, Train Acc: 0.9868, Val Loss: 0.0333, Val Acc: 0.9856\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/15: 100%|██████████| 261/261 [02:01<00:00,  2.15it/s, acc=0.497, loss=0.000853]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/15: Train Loss: 0.0205, Train Acc: 0.9954, Val Loss: 0.0271, Val Acc: 0.9866\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/15: 100%|██████████| 261/261 [01:59<00:00,  2.19it/s, acc=0.332, loss=0.00286] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/15: Train Loss: 0.0154, Train Acc: 0.9957, Val Loss: 0.0434, Val Acc: 0.9856\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/15: 100%|██████████| 261/261 [01:58<00:00,  2.19it/s, acc=0.25, loss=0.00852]   \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/15: Train Loss: 0.0065, Train Acc: 0.9988, Val Loss: 0.0199, Val Acc: 0.9914\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/15: 100%|██████████| 261/261 [03:37<00:00,  1.20it/s, acc=0.2, loss=0.00124]    \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/15: Train Loss: 0.0067, Train Acc: 0.9983, Val Loss: 0.0549, Val Acc: 0.9789\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/15: 100%|██████████| 261/261 [03:00<00:00,  1.45it/s, acc=0.167, loss=0.00135]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/15: Train Loss: 0.0028, Train Acc: 0.9998, Val Loss: 0.0325, Val Acc: 0.9866\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/15: 100%|██████████| 261/261 [02:00<00:00,  2.17it/s, acc=0.143, loss=0.00362]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/15: Train Loss: 0.0031, Train Acc: 0.9995, Val Loss: 0.0209, Val Acc: 0.9914\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/15: 100%|██████████| 261/261 [02:01<00:00,  2.15it/s, acc=0.125, loss=0.000201] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/15: Train Loss: 0.0045, Train Acc: 0.9983, Val Loss: 0.0315, Val Acc: 0.9875\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10/15: 100%|██████████| 261/261 [02:00<00:00,  2.17it/s, acc=0.111, loss=0.00171]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/15: Train Loss: 0.0018, Train Acc: 0.9995, Val Loss: 0.0342, Val Acc: 0.9885\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11/15: 100%|██████████| 261/261 [02:00<00:00,  2.16it/s, acc=0.0999, loss=0.000914]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/15: Train Loss: 0.0028, Train Acc: 0.9993, Val Loss: 0.0299, Val Acc: 0.9875\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12/15: 100%|██████████| 261/261 [02:00<00:00,  2.17it/s, acc=0.0909, loss=0.00433] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/15: Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0264, Val Acc: 0.9875\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13/15: 100%|██████████| 261/261 [01:59<00:00,  2.19it/s, acc=0.0833, loss=0.00495] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/15: Train Loss: 0.0030, Train Acc: 0.9993, Val Loss: 0.0394, Val Acc: 0.9866\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14/15: 100%|██████████| 261/261 [02:00<00:00,  2.16it/s, acc=0.0769, loss=0.00084] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14/15: Train Loss: 0.0013, Train Acc: 0.9998, Val Loss: 0.0287, Val Acc: 0.9904\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 15/15: 100%|██████████| 261/261 [01:59<00:00,  2.19it/s, acc=0.0714, loss=1.86e-5] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/15: Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0316, Val Acc: 0.9875\n"]}],"source":["# Define data transformations\n","data_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load the datasets\n","train_data = ImageFolder(train_dir, transform=data_transforms)\n","test_data = ImageFolder(test_dir, transform=data_transforms)\n","\n","# Split the train_data into train and validation sets\n","train_size = int(0.8 * len(train_data))\n","val_size = len(train_data) - train_size\n","train_data, val_data = random_split(train_data, [train_size, val_size])\n","\n","# Create DataLoaders for the datasets\n","batch_size = 16\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the ResNet-152 model\n","model = models.resnet152(pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Training loop with progress bar, loss, accuracy, validation loss, and validation accuracy\n","epochs = 15\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n","\n","    for inputs, labels in progress_bar:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","\n","        # Update the progress bar\n","        progress_bar.set_postfix(loss=loss.item(), acc=torch.true_divide(running_corrects, ((epoch * len(train_loader.dataset)) + inputs.size(0))).item())\n","\n","    # Calculate epoch loss and accuracy\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n","\n","    # Validation loop\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_running_corrects = 0\n","\n","    for inputs, labels in val_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(inputs)\n","            val_loss = criterion(outputs, labels)\n","            _, preds = torch.max(outputs, 1)\n","\n","        val_running_loss += val_loss.item() * inputs.size(0)\n","        val_running_corrects += torch.sum(preds == labels.data)\n","\n","    # Calculate validation loss and accuracy\n","    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n","    val_epoch_acc = val_running_corrects.double() / len(val_loader.dataset)\n","\n","    # Print epoch results\n","    print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T21:56:44.559389Z","iopub.status.busy":"2023-05-03T21:56:44.559062Z","iopub.status.idle":"2023-05-03T21:56:57.160373Z","shell.execute_reply":"2023-05-03T21:56:57.159122Z","shell.execute_reply.started":"2023-05-03T21:56:44.559359Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 87.18%\n"]}],"source":["# Test the model\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy:.2f}%\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), '/kaggle/working/resnet152_chest_xray_pneumonia.pth')"]},{"cell_type":"markdown","metadata":{},"source":["RESNET 200 Custom Model"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T23:06:29.885153Z","iopub.status.busy":"2023-05-03T23:06:29.884700Z","iopub.status.idle":"2023-05-03T23:41:07.964951Z","shell.execute_reply":"2023-05-03T23:41:07.964045Z","shell.execute_reply.started":"2023-05-03T23:06:29.885118Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=252, loss=0.454] \n","Epoch 2/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.835, loss=0.652] \n","Epoch 3/15: 100%|██████████| 261/261 [02:15<00:00,  1.92it/s, acc=0.45, loss=0.0222] \n","Epoch 4/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.307, loss=0.295]  \n","Epoch 5/15: 100%|██████████| 261/261 [02:16<00:00,  1.91it/s, acc=0.234, loss=0.0204] \n","Epoch 6/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.187, loss=0.148]   \n","Epoch 7/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.158, loss=0.0785]  \n","Epoch 8/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.136, loss=0.144]   \n","Epoch 9/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.119, loss=0.0167]  \n","Epoch 10/15: 100%|██████████| 261/261 [02:16<00:00,  1.91it/s, acc=0.106, loss=0.147]    \n","Epoch 11/15: 100%|██████████| 261/261 [02:15<00:00,  1.93it/s, acc=0.0954, loss=0.0483] \n","Epoch 12/15: 100%|██████████| 261/261 [02:16<00:00,  1.91it/s, acc=0.0881, loss=0.0328]  \n","Epoch 13/15: 100%|██████████| 261/261 [02:16<00:00,  1.91it/s, acc=0.0807, loss=0.0568] \n","Epoch 14/15: 100%|██████████| 261/261 [02:16<00:00,  1.91it/s, acc=0.0744, loss=0.00303] \n","Epoch 15/15: 100%|██████████| 261/261 [02:19<00:00,  1.87it/s, acc=0.0695, loss=0.298]   \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/15: Train Loss: 0.0783, Train Acc: 0.9734, Val Loss: 0.0894, Val Acc: 0.9751\n","Accuracy on test set: 83.01%\n"]}],"source":["import os\n","import torch\n","import torchvision\n","import numpy as np\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.datasets import ImageFolder\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torchvision.models import resnet\n","\n","# Custom ResNet-200 model\n","class CustomBottleneck(resnet.Bottleneck):\n","    expansion = 4\n","\n","def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n","    model = resnet.ResNet(block, layers, **kwargs)\n","    return model\n","\n","def resnet200(pretrained=False, progress=True, **kwargs):\n","    return _resnet('resnet200', CustomBottleneck, [3, 24, 36, 3], pretrained, progress, **kwargs)\n","\n","# Define dataset paths\n","data_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray'\n","train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')\n","\n","# Define data transformations\n","data_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load the datasets\n","train_data = ImageFolder(train_dir, transform=data_transforms)\n","test_data = ImageFolder(test_dir, transform=data_transforms)\n","\n","# Split the train_data into train and validation sets\n","train_size = int(0.8 * len(train_data))\n","val_size = len(train_data) - train_size\n","train_data, val_data = random_split(train_data, [train_size, val_size])\n","\n","# Create DataLoaders for the datasets\n","batch_size = 16\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the custom ResNet-200 model\n","model = resnet200()\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Training and validation loop\n","epochs = 15\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n","\n","    for inputs, labels in progress_bar:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","\n","        # Update the progress bar\n","        progress_bar.set_postfix(loss=loss.item(), acc=torch.true_divide(running_corrects, ((epoch * len(train_loader.dataset)) + inputs.size(0))).item())\n","\n","    # Calculate epoch loss and accuracy\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_acc = running_corrects.double() /len(train_loader.dataset)\n","# Validation loop\n","model.eval()\n","val_running_loss = 0.0\n","val_running_corrects = 0\n","\n","for inputs, labels in val_loader:\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(inputs)\n","        val_loss = criterion(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","    val_running_loss += val_loss.item() * inputs.size(0)\n","    val_running_corrects += torch.sum(preds == labels.data)\n","\n","# Calculate validation loss and accuracy\n","val_epoch_loss = val_running_loss / len(val_loader.dataset)\n","val_epoch_acc = val_running_corrects.double() / len(val_loader.dataset)\n","\n","# Print epoch results\n","print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n","\n","# Test the model\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy:.2f}%\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), '/kaggle/working/resnet200_chest_xray_pneumonia.pth')\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Report for Resnet 200"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T00:06:22.944092Z","iopub.status.busy":"2023-05-04T00:06:22.943759Z","iopub.status.idle":"2023-05-04T00:06:37.647535Z","shell.execute_reply":"2023-05-04T00:06:37.646537Z","shell.execute_reply.started":"2023-05-04T00:06:22.944060Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 83.01%\n","Classification Report for RESNET 200:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.56      0.71       234\n","           1       0.79      0.99      0.88       390\n","\n","    accuracy                           0.83       624\n","   macro avg       0.88      0.78      0.80       624\n","weighted avg       0.86      0.83      0.82       624\n","\n","Confusion Matrix:\n","[[131 103]\n"," [  3 387]]\n"]}],"source":["# Custom ResNet-200 model definition\n","class CustomBottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):\n","        super(CustomBottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = norm_layer(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=dilation, bias=False, groups=groups, dilation=dilation)\n","        self.bn2 = norm_layer(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = norm_layer(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n","    model = torchvision.models.ResNet(block, layers, **kwargs)\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n","        model.load_state_dict(state_dict)\n","    return model\n","\n","def resnet200(pretrained=False, progress=True, **kwargs):\n","    return _resnet('resnet200', CustomBottleneck, [3, 24, 36, 3], pretrained, progress, **kwargs)\n","\n","# Load the custom ResNet-200 model\n","model = resnet200(pretrained=False)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","model.load_state_dict(torch.load('resnet200_chest_xray_pneumonia.pth'))\n","model = model.to(device)\n","# Test the model\n","model.eval()\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        \n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(predicted.cpu().numpy())\n","\n","# Calculate accuracy\n","accuracy = np.mean(np.array(all_labels) == np.array(all_preds)) * 100\n","print(f\"Accuracy on test set: {accuracy:.2f}%\")\n","\n","# Generate classification report and confusion matrix\n","report = classification_report(all_labels, all_preds, target_names=['0', '1'])\n","conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","print(\"Classification Report for RESNET 200:\")\n","print(report)\n","\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n"]},{"cell_type":"markdown","metadata":{},"source":["Report for RESNET 152"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T23:56:25.800246Z","iopub.status.busy":"2023-05-03T23:56:25.799394Z","iopub.status.idle":"2023-05-03T23:56:39.869699Z","shell.execute_reply":"2023-05-03T23:56:39.868753Z","shell.execute_reply.started":"2023-05-03T23:56:25.800204Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 87.18%\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.66      0.79       234\n","           1       0.83      1.00      0.91       390\n","\n","    accuracy                           0.87       624\n","   macro avg       0.91      0.83      0.85       624\n","weighted avg       0.89      0.87      0.86       624\n","\n","Confusion Matrix:\n","[[155  79]\n"," [  1 389]]\n"]}],"source":["import torch\n","import os\n","import numpy as np\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Define dataset paths\n","data_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray'\n","test_dir = os.path.join(data_dir, 'test')\n","\n","# Define data transformations\n","data_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load the test dataset\n","test_data = ImageFolder(test_dir, transform=data_transforms)\n","test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n","\n","# Load the model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = models.resnet152(pretrained=False)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","model.load_state_dict(torch.load('resnet152_chest_xray_pneumonia.pth'))\n","model = model.to(device)\n","\n","# Test the model\n","model.eval()\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        \n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(predicted.cpu().numpy())\n","\n","# Calculate accuracy\n","accuracy = np.mean(np.array(all_labels) == np.array(all_preds)) * 100\n","print(f\"Accuracy on test set: {accuracy:.2f}%\")\n","\n","# Generate classification report and confusion matrix\n","report = classification_report(all_labels, all_preds, target_names=['0', '1'])\n","conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","print(\"Classification Report for RESNET 152:\")\n","print(report)\n","\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
